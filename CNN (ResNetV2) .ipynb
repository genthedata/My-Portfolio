{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we train a convolution neural network (based upon ResNetv2) to classify routes by their grades and achieve 70% accuracy on the test dataset (using a one out accuracy, the true grade can be +- 1 of our guess).  We experiment with three different loss functions to try and take advantage of the ordering of our labels (grades can be arranged on a number line).  For our loss functions we use:\n",
    "- CJS (cummlative Jensen-Shannon divergence), https://arxiv.org/pdf/1708.07089.pdf\n",
    "- Squared earth mover's distance (or Wasserstein metric), https://arxiv.org/pdf/1708.07089.pdf\n",
    "- Cross-entropy loss (standard loss function for any classification problem, which ignores the orderings of labels)\n",
    "\n",
    "Our one out accuracy results are:\n",
    "\n",
    "|Loss function| Accuracy (one out)|\n",
    "|---|---|\n",
    "|CJS | 69.4%|\n",
    "|squared earth mover's distance| 70.8%|\n",
    "|cross-entropy| 64.2%|\n",
    "\n",
    "Convolutional neural network model: this model is based upon a 14 layer ResNetv2 but with a few key differences: we add dropout layers and add the scaling of the residuals as in Inception-ResNet (https://arxiv.org/pdf/1602.07261.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the data pipeline\n",
    "We load the input data, create the training, validation and testing datasets ensuring the proper distribution of grade 6, 7 and 8's in each, and then build the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.utils import io\n",
    "\n",
    "from skopt import gp_minimize, callbacks, load\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "# Need skopt.__version__ > 0.5.2 or pip install git+https://github.com/scikit-optimize/scikit-optimize/\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams[\"image.origin\"] = 'lower'\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "# ignore warnings about Tensorflow API v2.0\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (2575, 1, 18, 11) float32\n",
      "Train labels shape:  (2575,) int32\n",
      "Validation data shape:  (733, 1, 18, 11)\n",
      "Validation labels shape:  (733,)\n",
      "Test data shape:  (366, 1, 18, 11)\n",
      "Test labels shape:  (366,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(train=0.7, val=0.2, test=0.1, data_format='channels_first'):\n",
    "    \"\"\"\n",
    "    Loads the datasets from data/data.npz and randomly creates the train, test and \n",
    "    validation datasets.\n",
    "\n",
    "    Inputs:\n",
    "    - train, val, test: the fraction of the dataset in the train dataset, validation dataset\n",
    "      and test dataset respectively\n",
    "    - data_format: A string, one of 'channels_last' (default) or 'channels_first'.\n",
    "      'channels_last' best for CPU and 'channels_first' best for GPU\n",
    "    Returns:\n",
    "    - 6 numpy arrays: X_train, y_train, X_val, y_val, X_test, y_test in either\n",
    "      channel_last or channel_first format\n",
    "    - grade_dict : a dictionary of coverting the grades to numerical scores.\n",
    "    \"\"\"\n",
    "    # check fraction of datasets sum up to 1 (ignoring float rounding errors)\n",
    "    assert np.isclose(train + val + test, 1)\n",
    "\n",
    "    # load the data, n.b. arrays are sorted by grade\n",
    "    loaded = np.load('data/data_user.npz')\n",
    "    moves = loaded['moves']\n",
    "    grades = loaded['grades']\n",
    "    grade_dict = loaded['grade_dict'][()]\n",
    "    \n",
    "    # reduce the last axis - start, mid or ending hold\n",
    "    # Doesn't change anything to test accuracy\n",
    "    moves = np.sum(moves, axis=-1, keepdims=True)\n",
    "\n",
    "    # Find partition arguments between grade 6, 7 & 8\n",
    "    part_arg = np.searchsorted(grades, [grade_dict['7A'], grade_dict['8A']])\n",
    "\n",
    "    # now shuffle within the grade 6's, 7's and 8's\n",
    "    permute_idx = np.arange(grades.shape[0])\n",
    "    np.random.shuffle(permute_idx[:part_arg[0]])\n",
    "    np.random.shuffle(permute_idx[part_arg[0]:part_arg[1]])\n",
    "    np.random.shuffle(permute_idx[part_arg[1]:])\n",
    "    moves = moves[permute_idx]\n",
    "    grades = grades[permute_idx]\n",
    "\n",
    "    # data processing\n",
    "    if data_format == 'channels_first':\n",
    "        moves = np.moveaxis(moves, -1, 1)\n",
    "    moves = moves.astype(np.float32)\n",
    "\n",
    "    # create the train, val and test datasets from the grade classes\n",
    "    part_start = np.append(0, part_arg)\n",
    "    size = np.array([part_arg[0], part_arg[1] - part_arg[0],\n",
    "                     len(grades) - part_arg[1]])\n",
    "\n",
    "    num_val = (val * size).astype(int)\n",
    "    num_test = (test * size).astype(int)\n",
    "    num_train = (size - num_val - num_test).astype(int)\n",
    "\n",
    "    # generate the training, val and test sets\n",
    "    slice_range = [part_start,\n",
    "                   part_start + num_train,\n",
    "                   part_start + num_train + num_val,\n",
    "                   part_start + num_train + num_val + num_test]\n",
    "    X, y = [], []\n",
    "    for j in range(3):\n",
    "        grade_list, moves_list = [], []\n",
    "        for i in range(3):\n",
    "            grade_list.append(grades[slice_range[j][i]: slice_range[j+1][i]])\n",
    "            moves_list.append(moves[slice_range[j][i]: slice_range[j+1][i]])\n",
    "        X.append(np.concatenate(moves_list))\n",
    "        y.append(np.concatenate(grade_list))\n",
    "\n",
    "    X_train, X_val, X_test = X\n",
    "    y_train, y_val, y_test = y\n",
    "\n",
    "    # check: sets are the correct length\n",
    "    assert (len(y_val) == np.sum(num_val) and len(y_test) == np.sum(num_test)\n",
    "            and len(y_train) == np.sum(num_train))\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, grade_dict\n",
    "\n",
    "\n",
    "data_format = 'channels_first'\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, grade_dict = load_data()\n",
    "num_classes = len(grade_dict) - 1\n",
    "\n",
    "print('Train data shape: ', X_train.shape, X_train.dtype)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "\n",
    "def construct_datasets(num_epochs=1):\n",
    "    \"\"\"\n",
    "    Constructs the datasets in Tensorflow.\n",
    "\n",
    "    Inputs: \n",
    "    - num_epochs: The number of epochs to run the training data for\n",
    "\n",
    "    Outputs:\n",
    "    - next_element_train, next_element_test: get_next() method for train and test dataset iterators.\n",
    "      The next_element_test is either from the validation or testing dataset depending on which has\n",
    "      been initialised\n",
    "    - train_init_op, val_init_op, test_init_op:\n",
    "      iterator initialisation operations for the respective datasets\n",
    "    - steps_to_epochs: The number of integers steps to each epoch of the training dataset\n",
    "    \"\"\"\n",
    "    prefetch = 2\n",
    "\n",
    "    # make sure the dataset is on the CPU to leave the GPU for training the model\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.variable_scope('train_dataset'):\n",
    "            dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "                (X_train, y_train))\n",
    "            dataset_train = dataset_train.apply(\n",
    "                tf.data.experimental.shuffle_and_repeat(len(X_train), count=num_epochs))\n",
    "            dataset_train = dataset_train.shuffle(len(X_train))\n",
    "            dataset_train = dataset_train.batch(batch_size).prefetch(prefetch)\n",
    "\n",
    "        with tf.variable_scope('validation_dataset'):\n",
    "            dataset_val = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            dataset_val = dataset_val.batch(batch_size).prefetch(prefetch)\n",
    "        with tf.variable_scope('test_dataset'):\n",
    "            dataset_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            dataset_test = dataset_test.batch(batch_size).prefetch(prefetch)\n",
    "\n",
    "        iterator_train = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                                         dataset_train.output_shapes)\n",
    "        next_element_train = iterator_train.get_next()\n",
    "        iterator_test = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                                        dataset_train.output_shapes)\n",
    "        next_element_test = iterator_test.get_next()\n",
    "\n",
    "        train_init_op = iterator_train.make_initializer(dataset_train)\n",
    "        val_init_op = iterator_test.make_initializer(dataset_val)\n",
    "        test_init_op = iterator_test.make_initializer(dataset_test)\n",
    "        steps_to_epochs = len(X_train) // batch_size\n",
    "\n",
    "    return next_element_train, next_element_test, train_init_op, val_init_op, test_init_op, steps_to_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network\n",
    "\n",
    "Our neural network is a deep network based upon Resnetv2 and has the same structure as the CIFAR-10 version of ResNetv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ResNetv2(inputs, is_training, total_layers=20, num_classes=10, reg=2e-4,\n",
    "                   drop_rate=0.5, data_format='channels_first', scaling=False):\n",
    "    \"\"\"\n",
    "    Creates a ResNetv2 model based upon CIFAR-10 ResNet.  \n",
    "    Total_layers = 6n + 2\n",
    "    \"\"\"\n",
    "    assert (total_layers - 2) % 6 == 0\n",
    "    num_layers = (total_layers - 2) // 6\n",
    "    filters = [16, 32, 64]\n",
    "    \n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    # Helper layer functions\n",
    "    def batch_norm_relu_conv2d(inputs, filters, stride=1):\n",
    "        inputs = batch_norm_relu(inputs)\n",
    "        inputs = conv2d(inputs, filters, stride=stride)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def batch_norm_relu(inputs):\n",
    "        inputs = tf.layers.batch_normalization(inputs, training=is_training)\n",
    "        return tf.nn.relu(inputs)\n",
    "\n",
    "\n",
    "    def conv2d(inputs, filters, kernel_size=3, stride=1):\n",
    "        inputs = tf.layers.conv2d(inputs, filters, kernel_size, strides=stride, padding=\"same\", kernel_initializer=initializer,\n",
    "                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(\n",
    "                                      scale=reg),\n",
    "                                  data_format=data_format)\n",
    "        if drop_rate != 0:\n",
    "            # n.b rate = 1 - keep_prob\n",
    "            inputs = tf.layers.dropout(inputs, rate=drop_rate, training=is_training)\n",
    "        return inputs\n",
    "\n",
    "    # Resnet unit\n",
    "    def ResNet_unit(inputs, filters, i, j, subsample):\n",
    "        with tf.variable_scope(f\"conv{i+2}_{j+1}\"):\n",
    "            shortcut = inputs\n",
    "            stride = 2 if subsample else 1\n",
    "\n",
    "            # for the first unit batch_norm_relu before splitting into two paths\n",
    "            if i == 0 and j == 0:\n",
    "                inputs = batch_norm_relu(inputs)\n",
    "                shortcut = inputs\n",
    "                inputs = conv2d(inputs, filters, stride=stride)\n",
    "            else:\n",
    "                inputs = batch_norm_relu_conv2d(inputs, filters, stride=stride)\n",
    "            inputs = batch_norm_relu_conv2d(inputs, filters)\n",
    "\n",
    "            if subsample:\n",
    "                if data_format == 'channels_last':\n",
    "                    paddings = tf.constant(\n",
    "                        [[0, 0], [0, 0], [0, 0], [0, filters // 2]])\n",
    "                    # reduce image height and width by striding as in resnet paper\n",
    "                    shortcut = shortcut[:, ::2, ::2, :]\n",
    "                else:\n",
    "                    paddings = tf.constant(\n",
    "                        [[0, 0], [0, filters // 2], [0, 0], [0, 0]])\n",
    "                    shortcut = shortcut[:, :, ::2, ::2]\n",
    "                shortcut = tf.pad(shortcut, paddings)\n",
    "            # scale activation ala Inception-ResNet\n",
    "            if scaling is True:\n",
    "                inputs = 0.2 * inputs\n",
    "            inputs = shortcut + inputs\n",
    "\n",
    "            return inputs\n",
    "    \n",
    "    # Construct ResNet\n",
    "\n",
    "    # first do a single convolution ResNet_unit with no addition\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        inputs = conv2d(inputs, filters[0], stride=2)\n",
    "\n",
    "    # now some ResNet units\n",
    "    for i in range(3):\n",
    "        for j in range(num_layers):\n",
    "            # don't subsample on first go round\n",
    "            subsample = i > 0 and j == 0\n",
    "            inputs = ResNet_unit(inputs, filters[i], i, j, subsample)\n",
    "    \n",
    "    with tf.variable_scope(\"pool_and_fc\"):\n",
    "        # Final activation\n",
    "        inputs = tf.nn.relu(inputs)\n",
    "\n",
    "        # Global average pooling, 10 way FC layer and then output to scores.\n",
    "        # Global average pooling is same as doing reduce_mean\n",
    "        if data_format == 'channels_last':\n",
    "            reduce_axis = [1, 2]\n",
    "        else:\n",
    "            reduce_axis = [2, 3]\n",
    "        inputs = tf.reduce_mean(inputs, axis=reduce_axis)\n",
    "        inputs = tf.layers.flatten(inputs)\n",
    "        inputs = tf.layers.batch_normalization(inputs, training=is_training)\n",
    "        scores = tf.layers.dense(inputs, num_classes, kernel_initializer=initializer,\n",
    "                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(\n",
    "                                     scale=reg))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test to check that our neutral network works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 15)\n"
     ]
    }
   ],
   "source": [
    "def test_model_ResNet_fc():\n",
    "    \"\"\" A small unit test for model_ResNetv2 above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.zeros((50, 3, 18, 11))\n",
    "    scores = model_ResNetv2(x, 1, num_classes=num_classes, drop_rate=0.8,\n",
    "                            data_format='channels_first', scaling=True)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "\n",
    "test_model_ResNet_fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_earth_mover(labels, logits, p=2):\n",
    "    \"\"\"\n",
    "    Computes the normalised squared Earth Mover’s Distance loss from https://arxiv.org/pdf/1611.05916.pdf.\n",
    "    Since our classes are ordered this loss behaves much better than the usual softmax cross entropy.\n",
    "\n",
    "    Inputs:\n",
    "    - labels: Tensor of shape [batch_size] and dtype int32 or int64.\n",
    "      Each entry in labels must be an index in [0, num_classes)\n",
    "    - logits: Unscaled log probabilities of shape [batch_size, num_classes]\n",
    "    - p: which l^p norm to use. p = 2 represents the squared Earth mover's distance\n",
    "\n",
    "    Returns:\n",
    "    - loss: A Tensor of the same shape as labels and of the same type as logits with the softmax cross entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(\"sparse_earth_mover\"):\n",
    "        num_classes = tf.shape(logits)[-1]\n",
    "\n",
    "        logits_normed = tf.nn.softmax(logits)\n",
    "        one_hot_labels = tf.one_hot(labels, num_classes)\n",
    "\n",
    "        cdf_labels = tf.cumsum(one_hot_labels, axis=-1)\n",
    "        cdf_logits = tf.cumsum(logits_normed, axis=-1)\n",
    "        if p == 2:\n",
    "            loss = tf.sqrt(tf.reduce_mean(\n",
    "                tf.square(cdf_labels - cdf_logits), axis=-1))\n",
    "        if p == 1:\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.abs(cdf_labels - cdf_logits), axis=-1)\n",
    "        else:\n",
    "            loss = (tf.reduce_mean(\n",
    "                (cdf_labels - cdf_logits) ** p, axis=-1)) ** (1.0 / p)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CJS(labels, logits):\n",
    "    \"\"\"Computes the symmetrical discrete cumulative Jensen-Shannon divergence from https://arxiv.org/pdf/1708.07089.pdf\n",
    "\n",
    "    Inputs:\n",
    "    - labels: Tensor of shape [batch_size] and dtype int32 or int64.\n",
    "      Each entry in labels must be an index in [0, num_classes)\n",
    "    - logits: Unscaled log probabilities of shape [batch_size, num_classes]\n",
    "\n",
    "    Returns:\n",
    "    - loss: A Tensor of the same shape as labels and of the same type as logits with the softmax cross entropy loss.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"CJS_loss\"):\n",
    "        num_classes = tf.shape(logits)[-1]\n",
    "\n",
    "        logits_normed = tf.nn.softmax(logits)\n",
    "        one_hot_labels = tf.one_hot(labels, num_classes)\n",
    "\n",
    "        cdf_labels = tf.cumsum(one_hot_labels, axis=-1)\n",
    "        cdf_logits = tf.cumsum(logits_normed, axis=-1)\n",
    "\n",
    "        def ACCJS(p, q):\n",
    "            with tf.name_scope(\"ACCJS\"):\n",
    "            # if p(i) = 0 then ACCJS(p, q)(i) = 0 since xlog(x) -> 0 as x-> 0\n",
    "                p = tf.clip_by_value(p, 1e-10, 1.0)\n",
    "                return 0.5 * tf.reduce_sum(p * tf.log(p / (0.5 * (p + q))), axis=-1)\n",
    "\n",
    "        loss = ACCJS(cdf_logits, cdf_labels) + ACCJS(cdf_labels, cdf_logits)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc_test(sess, x, next_element, scores, is_training):\n",
    "    \"\"\"\n",
    "    Checks the accuracy of a classification model.\n",
    "\n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - next_element: A TensorFlow placeholder Tensor where the next batch of elements will be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "    - A TensorFlow placeholder Tensor where a bool should be fed if we are training the dataset\n",
    "\n",
    "    Returns: Accuracy of the model\n",
    "    \"\"\"\n",
    "    exact, top3, one_out, num_samples = [0.0] * 4\n",
    "    with tf.name_scope('accuracy'):\n",
    "        while True:\n",
    "            try:\n",
    "                (x_np, y_np) = sess.run(next_element)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            feed_dict = {x: x_np, is_training: False}\n",
    "            scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "            num_samples += x_np.shape[0]\n",
    "            # find top 3 and top 1 predictions (nb argpartition doesn't sort)\n",
    "            pred_top3 = np.argpartition(scores_np, -3, axis=-1)[:, -3:]\n",
    "            pred_exact = scores_np.argmax(axis=-1)\n",
    "            # add num correct\n",
    "            # add extra dimension to y_np to broadcast\n",
    "            top3 += np.sum((pred_top3 - y_np[:, None]) == 0)\n",
    "            one_out += np.sum(np.abs(pred_exact - y_np) <= 1)\n",
    "            exact += np.sum(pred_exact == y_np)\n",
    "        acc_top3 = top3 / num_samples\n",
    "        acc_one_out = one_out / num_samples\n",
    "        acc_exact = exact / num_samples\n",
    "    return acc_exact, acc_one_out, acc_top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc_train(x, y, scores):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model from a batch of data.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - y: A TensorFlow placeholder Tensor where input classification scores\n",
    "      should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "\n",
    "    Returns: Accuracy of the model on a batch of training data\n",
    "    \"\"\"\n",
    "    with tf.name_scope('accuracy'):\n",
    "        num_samples = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "        \n",
    "        top3 = tf.count_nonzero(tf.nn.in_top_k(scores, y, 3))\n",
    "        y_pred = tf.argmax(scores, axis=1, output_type=tf.int32)\n",
    "        one_out = tf.count_nonzero(tf.abs(y_pred - y) <= 1)\n",
    "        exact = tf.count_nonzero(tf.equal(y_pred, y))\n",
    "        \n",
    "        # calculate accuracies\n",
    "        acc_top3 = tf.cast(top3, tf.float32) / num_samples\n",
    "        acc_one_out = tf.cast(one_out, tf.float32) / num_samples\n",
    "        acc_exact = tf.cast(exact, tf.float32) / num_samples\n",
    "    return acc_exact, acc_one_out, acc_top3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_init_fn, optimizer_init_fn, loss_fn, lr, num_epochs=1,\n",
    "          decay_at=[], decay_to=[], experiment_name=\"\",\n",
    "          save=False, log=True, save_graph=False, val=True):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.layers. It trains\n",
    "    a model for num_epochs, peridoically checks the accuracy on the validation\n",
    "    dataset, logs the training data to Tensorboard, saves the graph, and tests \n",
    "    the final accuracy on the test dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    - data_format: Channels first or last for the tensors\n",
    "    - experiment_nume: The name to call the experiement when logging and saving\n",
    "    - deacy_at: A list of epochs to decay the learning rate at\n",
    "    - decay_to: A list of learning rates to decay to\n",
    "    - save: A bool to decide if we save the Tensorflow graph after training the model\n",
    "    - log: A bool to decide to log the training for Tensorboard\n",
    "    - save_graph: A bool to decide if we save the computational graph for Tensorboard\n",
    "    - val: A bool to decide if we check the accuracy on the validation data or test data.\n",
    "      Set to False if there is no validation dataset.\n",
    "\n",
    "    Returns:\n",
    "    - acc: List of length 3 which is the accuracy on the validation or test dataset,\n",
    "      see val parameter.  acc[0] is exact accuracy, acc[1] is one out accuracy and\n",
    "      acc[2] is top 3 accuracy.\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # construct the datasets\n",
    "    (next_element_train, next_element_test, train_init_op,\n",
    "     val_init_op, test_init_op, steps_to_epochs) \\\n",
    "        = construct_datasets(num_epochs)\n",
    "    (x, y) = next_element_train\n",
    "\n",
    "    # declare placeholders\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    lr_var = tf.Variable(lr, trainable=False, name='learning_rate')\n",
    "\n",
    "    # Whenever we need to record the loss, feed the test accuracy to these placeholders\n",
    "    with tf.name_scope('acc'):\n",
    "        tf_acc_ph_1out = tf.placeholder(tf.float32, shape=None)\n",
    "        tf_acc_ph_exact = tf.placeholder(tf.float32, shape=None)\n",
    "        tf_acc_ph_top3 = tf.placeholder(tf.float32, shape=None)\n",
    "        # Create a scalar summary object for the accuracy so it can be displayed\n",
    "        tf.summary.scalar(\"accuracy_within_1\", tf_acc_ph_1out)\n",
    "        tf.summary.scalar(\"accuracy_exact\", tf_acc_ph_exact)\n",
    "        tf.summary.scalar(\"accuracy_top3\", tf_acc_ph_top3)\n",
    "\n",
    "    # Use the model function to build the forward pass.\n",
    "    scores = model_init_fn(x, is_training)\n",
    "\n",
    "    # Compute the losses\n",
    "    loss_scores = loss_fn(labels=y, logits=scores)\n",
    "    loss_scores = tf.reduce_mean(loss_scores)\n",
    "    loss_reg = tf.losses.get_regularization_loss()\n",
    "    loss = loss_scores + loss_reg\n",
    "\n",
    "    # Tensorboard logging scalars\n",
    "    tf.summary.scalar('loss_scores', loss_scores)\n",
    "    tf.summary.scalar('loss_reg', loss_reg)\n",
    "    tf.summary.scalar('total_loss', loss)\n",
    "    tf.summary.scalar('learning_rate', lr_var)\n",
    "\n",
    "    # initialise the optimizer and create the training operation\n",
    "    optimizer = optimizer_init_fn(lr_var)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.name_scope('train'):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # check train accuarcy function\n",
    "    acc_train_op = check_acc_train(x, y, scores)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Tensorboard, merge all summaries but the error ones\n",
    "        merged = tf.summary.merge_all(scope=\"(?!acc)\")\n",
    "        merged_acc = tf.summary.merge_all(scope=\"(acc)\")\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Create the saver and Tensorboard log writers\n",
    "        if save:\n",
    "            saver = tf.train.Saver()\n",
    "        if log:\n",
    "            log_path = \"C:/tmp/logs\"\n",
    "            if save_graph:\n",
    "                train_writer = tf.summary.FileWriter(\n",
    "                    log_path + '/train/' + experiment_name, sess.graph)\n",
    "            else:\n",
    "                train_writer = tf.summary.FileWriter(\n",
    "                    log_path + '/train/' + experiment_name)\n",
    "            test_writer = tf.summary.FileWriter(\n",
    "                log_path + '/test/' + experiment_name)\n",
    "\n",
    "        # Initialize an iterator over the training dataset.\n",
    "        sess.run(train_init_op)\n",
    "        t = 0\n",
    "        while True:\n",
    "            # decay learning rate\n",
    "            if (t / steps_to_epochs) in decay_at:\n",
    "                lr_var.load(\n",
    "                    decay_to[decay_at.index(t / steps_to_epochs)], sess)\n",
    "\n",
    "            # train on next batch of data\n",
    "            feed_dict = {is_training: True}\n",
    "            try:\n",
    "                # check running accuracy on training batch and add to tensorboard every 20 steps\n",
    "                if (t + 1) % 20 == 0:\n",
    "                    summary, _, acc_train = sess.run(\n",
    "                        [merged, train_op, acc_train_op], feed_dict=feed_dict)\n",
    "                    if log:\n",
    "                        train_writer.add_summary(summary, t)\n",
    "                        acc_feed_dict = {tf_acc_ph_exact: acc_train[0],\n",
    "                                         tf_acc_ph_1out: acc_train[1],\n",
    "                                         tf_acc_ph_top3: acc_train[2]}\n",
    "                        train_writer.add_summary(\n",
    "                            sess.run(merged_acc, feed_dict=acc_feed_dict), t)\n",
    "                else:\n",
    "                    # train normally\n",
    "                    loss_np, _ = sess.run(\n",
    "                        [loss, train_op], feed_dict=feed_dict)\n",
    "                    # stop training if loss blows up\n",
    "                    if np.isnan(loss_np):\n",
    "                        if val:\n",
    "                            # roughly expected accuracy from random guess\n",
    "                            return 0.2 * np.ones(3)\n",
    "                        else:\n",
    "                            break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            t += 1\n",
    "\n",
    "            # Check accuacry on validation dataset every epoch\n",
    "            if t % steps_to_epochs == 0 and log and val:\n",
    "                sess.run(val_init_op)\n",
    "                acc_val = check_acc_test(sess, x, next_element_test, scores, is_training)\n",
    "                acc_feed_dict = {tf_acc_ph_exact: acc_val[0],\n",
    "                                 tf_acc_ph_1out: acc_val[1],\n",
    "                                 tf_acc_ph_top3: acc_val[2]}\n",
    "                test_writer.add_summary(\n",
    "                    sess.run(merged_acc, feed_dict=acc_feed_dict), t)\n",
    "\n",
    "        # End of training.  Calculate accuracy on validation dataset\n",
    "        if val:\n",
    "            sess.run(val_init_op)\n",
    "            acc_val = check_acc_test(sess, x, next_element_test, scores, is_training)\n",
    "            acc_feed_dict = {tf_acc_ph_exact: acc_val[0],\n",
    "                             tf_acc_ph_1out: acc_val[1],\n",
    "                             tf_acc_ph_top3: acc_val[2]}\n",
    "            \n",
    "            if log:\n",
    "                test_writer.add_summary(\n",
    "                    sess.run(merged_acc, feed_dict=acc_feed_dict), t)\n",
    "        else:\n",
    "            acc_val = None\n",
    "\n",
    "        print('End of training')\n",
    "        # Save the graph to disk.\n",
    "        if save:\n",
    "            save_path = saver.save(sess, f\"C:/tmp/save/{experiment_name}.ckpt\")\n",
    "        if val:\n",
    "            print(f\"Validation accuracy is:\")\n",
    "            print(f\"Exact: {acc_val[0]}\")\n",
    "            print(f\"1 out: {acc_val[1]}\")\n",
    "            print(f\"Top 3: {acc_val[2]}\\n\")\n",
    "            return acc_val\n",
    "        else:\n",
    "            # Calculate accuracy on test dataset\n",
    "            sess.run(test_init_op)\n",
    "            acc_test = check_acc_test(sess, x, next_element_test, scores, is_training)\n",
    "            print(f\"Accuracy on the test dataset\")\n",
    "            print(f\"Exact: {acc_test[0]}\")\n",
    "            print(f\"1 out: {acc_test[1]}\")\n",
    "            print(f\"Top 3: {acc_test[2]}\")\n",
    "            return acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we optimiser our hyperparameters we check the model with some sensible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Validation accuracy is:\n",
      "Exact: 0.41336971350613916\n",
      "1 out: 0.6562073669849932\n",
      "Top 3: 0.7066848567530696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 250\n",
    "total_layers = 14\n",
    "learning_rate = 0.05\n",
    "reg = 2e-4\n",
    "drop_rate = 0.6 # set this prob of the neurons to 0\n",
    "# decay by 0.95 learning rate every 5 epochs\n",
    "decay_at = list(np.arange(50) * 5)\n",
    "decay_to = list(learning_rate * 0.95 ** np.arange(50))\n",
    "\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}\"\n",
    "        f\"_reg{reg}_drop{drop_rate}_adam_CJS_scale0.2_eps1e-3_decay_0.95-every5\")\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers, reg=reg, drop_rate=drop_rate,\n",
    "                          num_classes=num_classes, scaling=True)\n",
    "\n",
    "def optimizer_init_fn(lr):\n",
    "    return tf.train.AdamOptimizer(lr, epsilon=1e-3)\n",
    "\n",
    "acc_val = train(model_init_fn, optimizer_init_fn, CJS, learning_rate, \n",
    "                      num_epochs=num_epochs, experiment_name=name,\n",
    "                      decay_at=decay_at, decay_to=decay_to, save_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise the hyperparameters\n",
    "We use `scikit-optimize` to perform a random grid search and Gaussian process optimisation to find the best hyperparameters: `learning rate`, `reg` and `decay_rate`.  We train a 14 layer network over 250 epochs.  This optimisation process takes 30 minutes per loss function on my laptop.  We choose to use an Adam optimizer and decay the learning rate by multiplying by the decay_rate every 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 250\n",
    "total_layers = 14\n",
    "num_calls = 25\n",
    "drop_rate = 0.6 # set this prob of the neurons to 0\n",
    "dim_learning_rate = Real(low=1e-6, high=2e-1, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_reg = Real(low=1e-5, high=1e-1, prior='log-uniform',\n",
    "               name='reg')\n",
    "dim_decay_rate = Real(0.9, 1, name='decay_rate')\n",
    "dimensions = [dim_learning_rate, dim_reg, dim_decay_rate]\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def op_acc(learning_rate, reg, decay_rate):\n",
    "    # decay learning rate by 0.95 learning rate every 5 epochs\n",
    "    decay_at = list(np.arange(50) * 5)\n",
    "    decay_to = list(learning_rate * decay_rate ** np.arange(50))\n",
    "    \n",
    "    name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "            f\"_drop{drop_rate}_adam_em_gp_search_scale0.2_eps1e-3_decay_{decay_rate}-every5\")\n",
    "\n",
    "    def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "        return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                              reg=reg, num_classes=num_classes)\n",
    "\n",
    "    def optimizer_init_fn(lr):\n",
    "        return tf.train.AdamOptimizer(lr, epsilon=1e-3)\n",
    "\n",
    "    # Stop printing from train function\n",
    "    # (https://stackoverflow.com/questions/23610585/ipython-notebook-avoid-printing-within-a-function/23611571#23611571)\n",
    "    with io.capture_output() as captured:\n",
    "        acc_val = train(model_init_fn, optimizer_init_fn, sparse_earth_mover, learning_rate,\n",
    "                             num_epochs=num_epochs, experiment_name=name,\n",
    "                             decay_at=decay_at, decay_to=decay_to)\n",
    "    # optimise one_out accuaracy\n",
    "    return -acc_val[1]\n",
    "\n",
    "\n",
    "search_result = gp_minimize(func=op_acc, dimensions=dimensions, n_calls=num_calls,\n",
    "                            verbose=True, n_restarts_optimizer=20)\n",
    "plot_convergence(search_result)\n",
    "print(search_result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def op_acc(learning_rate, reg, decay_rate):\n",
    "    name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "            f\"_drop{drop_rate}_adam_CJS_gp_search_scale0.2_eps1e-3_decay_{decay_rate}-every5\")\n",
    "    decay_at = list(np.arange(50) * 5)\n",
    "    decay_to = list(learning_rate * decay_rate ** np.arange(50))\n",
    "\n",
    "    def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "        return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                              reg=reg, num_classes=num_classes)\n",
    "\n",
    "    def optimizer_init_fn(lr):\n",
    "        return tf.train.AdamOptimizer(lr, epsilon=1e-3)\n",
    "\n",
    "    # Stop printing from train function\n",
    "    with io.capture_output() as captured:\n",
    "        acc_val = train(model_init_fn, optimizer_init_fn, CJS, learning_rate,\n",
    "                             num_epochs=num_epochs, experiment_name=name,\n",
    "                             decay_at=decay_at, decay_to=decay_to)\n",
    "    # optimise one_out accuaracy\n",
    "    return -acc_val[1]\n",
    "\n",
    "\n",
    "search_result = gp_minimize(func=op_acc, dimensions=dimensions, n_calls=num_calls,\n",
    "                            verbose=True, n_restarts_optimizer=20)\n",
    "plot_convergence(search_result)\n",
    "print(search_result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def op_acc(learning_rate, reg, decay_rate):\n",
    "    name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "            f\"_drop{drop_rate}_adam_cross_entropy_gp_search_scale0.2_eps1e-3_decay_{decay_rate}-every5\")\n",
    "    decay_at = list(np.arange(50) * 5)\n",
    "    decay_to = list(learning_rate * decay_rate ** np.arange(50))\n",
    "\n",
    "    def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "        return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                              reg=reg, num_classes=num_classes)\n",
    "\n",
    "    def optimizer_init_fn(lr):\n",
    "        return tf.train.AdamOptimizer(lr, epsilon=1e-3)\n",
    "\n",
    "    # Stop printing from train function\n",
    "    with io.capture_output() as captured:\n",
    "        acc_val = train(model_init_fn, optimizer_init_fn, tf.nn.sparse_softmax_cross_entropy_with_logits, learning_rate,\n",
    "                             num_epochs=num_epochs, experiment_name=name,\n",
    "                             decay_at=decay_at, decay_to=decay_to)\n",
    "    # optimise one_out accuaracy\n",
    "    return -acc_val[1]\n",
    "\n",
    "\n",
    "search_result = gp_minimize(func=op_acc, dimensions=dimensions, n_calls=num_calls,\n",
    "                            verbose=True, n_restarts_optimizer=20)\n",
    "plot_convergence(search_result)\n",
    "print(search_result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results\n",
    "\n",
    "We run the models with their optimised hyperparameters and retrain the model with the validation data included in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, X_val))\n",
    "y_train = np.concatenate((y_train, y_val))\n",
    "decay_at = list(np.arange(50) * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Accuracy on the test dataset\n",
      "Exact: 0.4781420765027322\n",
      "1 out: 0.7076502732240437\n",
      "Top 3: 0.73224043715847\n"
     ]
    }
   ],
   "source": [
    "# Earth Mover\n",
    "[learning_rate, reg, decay_rate] = [0.01, 0.07, 0.92]\n",
    "decay_to = list(learning_rate * decay_rate ** np.arange(50))\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "        \"_scale0.2_eps1e-3_decay_{decay_rate}-every5\"\n",
    "        f\"_drop{drop_rate}_adam_em_test\")\n",
    "\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                          reg=reg, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def optimizer_init_fn(lr):\n",
    "    return tf.train.AdamOptimizer(lr)\n",
    "\n",
    "\n",
    "acc_test = train(model_init_fn, optimizer_init_fn, sparse_earth_mover, learning_rate, num_epochs=num_epochs,\n",
    "                 experiment_name=name, val=False, decay_at=decay_at, decay_to=decay_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Accuracy on the test dataset\n",
      "Exact: 0.43989071038251365\n",
      "1 out: 0.6939890710382514\n",
      "Top 3: 0.7131147540983607\n"
     ]
    }
   ],
   "source": [
    "# CJS\n",
    "[learning_rate, reg, decay_rate] = [0.02, 1e-05, 1.0]\n",
    "decay_to = list(learning_rate * decay_rate ** np.arange(50))\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "        \"_scale0.2_eps1e-3_decay_{decay_rate}-every5\"\n",
    "        f\"_drop{drop_rate}_adam_CJS_test\")\n",
    "\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                          reg=reg, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def optimizer_init_fn(lr):\n",
    "    return tf.train.AdamOptimizer(lr)\n",
    "\n",
    "\n",
    "acc_test = train(model_init_fn, optimizer_init_fn, CJS, learning_rate, num_epochs=num_epochs,\n",
    "                 experiment_name=name, val=False, decay_at=decay_at, decay_to=decay_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Accuracy on the test dataset\n",
      "Exact: 0.4426229508196721\n",
      "1 out: 0.6420765027322405\n",
      "Top 3: 0.7021857923497268\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy\n",
    "[learning_rate, reg, decay_rate] = [0.0007, 0.0004, 0.9]\n",
    "decay_to = list(learning_rate * decay_rate ** np.arange(50))\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "        \"_scale0.2_eps1e-3_decay_{decay_rate}-every5\"\n",
    "        f\"_drop{drop_rate}_adam_cross_test\")\n",
    "\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                          reg=reg, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def optimizer_init_fn(lr):\n",
    "    return tf.train.AdamOptimizer(lr)\n",
    "\n",
    "\n",
    "acc_test = train(model_init_fn, optimizer_init_fn, tf.nn.sparse_softmax_cross_entropy_with_logits,\n",
    "                 learning_rate, num_epochs=num_epochs,\n",
    "                 experiment_name=name, val=False, decay_at=decay_at, decay_to=decay_to)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
